#--------------------------------------
# compare the exact Hessian for the 
# TOPALS model to the IRLS matrix
# that Camarda uses as covariance
# 
# the point is to investigate whether
# covariance of TOPALS params is 
# properly estimated by either or
# both formulas
# 
# Conclusion: the "Camarda" matrix
# converges to the true Hessian, but
# is not identical to the Hessian in
# each step.  In other words, at a point
# where we're partially through the 
# iterations and moving toward a solution,
# the "Camarda matrix" is not the same
# as the approx. curvature of the objective
# function.  But when we're at optimum,
# it *does* equal the Hessian. 
#--------------------------------------

library(tidyverse)
library(splines)

# load example data ----
# retrieve example data and HMD-based standards
load('input_datasets.Rdata')


TOPALS_fit = function( N, D, std,
                       age_group_bounds   = 0:100,
                       knot_positions     = c(0,1,10,20,40,70), 
                       penalty_precision  = 2,
                       max_iter           = 50,
                       pause_at_iter      = 50,
                       alpha_tol          = .00005,
                       details            = FALSE) {
  
  require(splines)
  
  ## single years of age from 0 to (A-1)
  A   = length(std)
  age = 0:(A-1)
  
  ## B is an AxK matrix. Each column is a linear B-spline basis function
  B      = bs( age, knots=knot_positions, degree=1 )
  K = ncol(B) 
  
  D1 = diff( diag(K), diff=1)
  P  = penalty_precision * crossprod(D1)
  
  ## number and width of age groups
  G     = length(age_group_bounds)-1   
  nages = diff(age_group_bounds)
  
  ## weighting matrix for mortality rates (assumes uniform
  ## distribution of single-year ages within groups)
  W = matrix(0, nrow=G, ncol=A, 
             dimnames=list(head(age_group_bounds,-1) , age))
  
  offset = 0
  for (g in 1:G) {
    W[g, offset + 1:nages[g]] = 1/nages[g]
    offset = offset + nages[g]
  }
  
  ## log lik function (without scaling constant)
  Lik = function(alpha) {
    M = W %*% exp( std + B %*% alpha)
    likelihood = sum(D * log(M) - N * M)
    return( likelihood )
  }
  
  ## penalty function
  Pen = function(alpha) {
    penalty    = -1/2 * t(alpha) %*% P %*% alpha
    return( penalty )
  }
  
  
  #------------------------------------------------
  # iteration function: 
  # next alpha vector as a function of current alpha
  #------------------------------------------------
  next_alpha = function(alpha) {
    mu = as.vector( exp( std + B %*% alpha))
    M  = as.vector( W %*% mu)
    
    Dhat = N * M
    
    X = W %*% diag(mu) %*% B
    A = diag(N/M)
    
    y = (D-Dhat)/N + X %*% alpha
    
    updated_alpha = solve( t(X) %*% A %*% X + P, t(X) %*% A %*% y)
    return(as.vector(updated_alpha))
  }
  
  ## main iteration:     
  amat     = matrix(NA, nrow=max_iter, ncol=K)
  amat[1,] = rep(0, K)
  
  niter = 0
  repeat {
    niter          = niter + 1  # eg, 1st iteration
    
    this_i         = niter + 1  # eg, 2nd row of amat produced by 1st iter
    last_i         = niter      
    amat[this_i,]  = next_alpha( amat[last_i,] )  # update
    change         = amat[this_i,] - amat[last_i,]
    
    converge = all( abs(change) < alpha_tol )
    overrun  = (niter == max_iter)
    pause    = (niter == pause_at_iter)
    
    if (converge | overrun | pause) { break }
    
  } # repeat
  
  if (details | !converge | overrun | pause) {
    if (!converge) print('did not converge')
    if (overrun) print('exceeded maximum number of iterations')
    
    mu    = as.vector( exp(std + B %*% amat[niter,]))
    M     = as.vector( W %*% mu )
    dhat  = N * M
    
    X     = W %*% diag(mu) %*% B
    A     = diag(N/M)
    
    covar = solve( t(X) %*% A %*% X + P)
    
    final_i     = niter+1  # which row of amat has the final estimates?
    final_alpha = amat[final_i,]
    
    logm              = as.vector( std + B %*% final_alpha )
    
    
    return( list( niter             = niter,
                  alpha             = amat[1:final_i,],
                  final_alpha       = final_alpha,
                  D                 = D,
                  N                 = N,
                  age_group_bounds  = age_group_bounds,
                  knots             = knot_positions,
                  std               = std,
                  B                 = B,
                  W                 = W,
                  P                 = P,
                  logm              = logm,
                  mu                = exp(logm),
                  M                 = as.vector(W %*% mu),
                  Dhat              = N * M,
                  covar             = covar,
                  Lik               = apply(amat[1:final_i,], 1, Lik),
                  Pen               = apply(amat[1:final_i,], 1, Pen),
                  converge          = converge, 
                  maxiter           = overrun))
  } else return( amat[niter,]) 
  
} # TOPALS_fit



dsn = 2
N = data[dsn,] %>% pull(N) %>% unlist()
D = data[dsn,] %>% pull(D) %>% unlist()
L = data[dsn,] %>% pull(L) %>% unlist()
H = data[dsn,] %>% pull(H) %>% unlist()

boundaries = c(L, tail(H,1))
std = male_std


fit = TOPALS_fit( N, D, std, age_group_bounds = boundaries, 
                  pause_at_iter = 99, alpha_tol = 5e-7,
                  details=TRUE)

exactg = function(fit) {
  part1 = t(fit$B) %*% diag(fit$mu) %*% t(fit$W)
  part2 = diag(1/fit$M) %*% D - fit$N
  return(part1 %*% part2 - fit$P %*% fit$final_alpha)
}

z = function(j) {
  bj = as.vector(fit$B[,j])
  part1 = diag(bj) %*% diag(fit$mu) %*% t(fit$W) %*% 
              diag(1/fit$M) %*% (fit$D -fit$Dhat)
  
  part2 = diag(fit$mu) %*% t(fit$W) %*% 
           diag( as.vector(fit$W %*% diag(fit$mu) %*% bj )) %*%
           diag( (1/fit$M)^2) %*% fit$D
  
  return(part1 - part2)
}

Zstar = matrix(NA, 100, 7)

for (j in 1:7) Zstar[,j] = z(j)


exactH  = -solve( t(fit$B) %*% Zstar - fit$P)
approxH = fit$covar 

## simulate the Hessian


## log lik function (without scaling constant)
Lik = function(alpha) {
#  likelihood = sum(fit$D * log(fit$M) - fit$N * fit$M)
#  return( likelihood )

  return( sum(dpois(fit$D, fit$Dhat, log=TRUE)))
}

## penalty function
Pen = function(alpha) {
  penalty    = -1/2 * t(alpha) %*% fit$P %*% alpha
  return( penalty )
}

Q = function(a) {Lik(a) + Pen(a)}

delta = .001

gradient = function(a) {
  K = ncol(fit$B)
  dv = rep(NA,K)
  
  for (i in 1:K) {
    tmp1 = Q( a + delta/2*diag(K)[i,])
    tmp2 = Q( a - delta/2*diag(K)[i,])
    dv[i] = (tmp1-tmp2)/delta    
  }
  return(dv)
}

gradient(fit$final_alpha)

K = ncol(fit$B)
HH = matrix(NA,K,K)

for (i in 1:K) {
  tmp1 = gradient(fit$final_alpha + delta/2*diag(K)[i,] )
  tmp2 = gradient(fit$final_alpha - delta/2*diag(K)[i,] )
  HH[i,] = (tmp2-tmp1)/delta
}

gradient(fit$final_alpha)

HH

## numerical essian



(1e4 * exactH)  %>%  round()
(1e4 * approxH) %>%  round()

(1e4 * (approxH - exactH)) %>% round()


show = function(fit, hue='red', ti='',subti='', true_logm=NA) {
  
  df_grouped = data.frame(
    L = head( fit$age_group_bounds, -1),
    U = tail( fit$age_group_bounds, -1),
    N = fit$N,
    D = fit$D
  ) %>%
    mutate(logmx_obs = log(D/N))
  
  df_single  = data.frame(
    age=  seq(fit$std) - .50,  # 0.5, 1.5, ...
    std = fit$std,
    logmx_fit  = fit$logm
  )
  
  # simulate 10th and 90th pointwise intervals at each age, and e0
  CH  = t( chol( fit$covar ))
  m   = fit$final_alpha
  sim = replicate(10000, {
    a=m + CH %*% rnorm(7)
    as.vector(fit$std + fit$B %*% a)
  })
  
  df_single$Q10 = apply(sim,1,quantile,prob=.10)
  df_single$Q90 = apply(sim,1,quantile,prob=.90)
  
  e0 = function(logmx) {
    mx = exp(logmx)
    px = exp(-mx)
    lx = c(1,cumprod(px))
    ex = sum( head(lx,-1) + tail(lx,-1))/2
    return(ex)
  }
  
  esim = apply(sim,2,e0)
  e10  = quantile(esim,prob=.10)
  e90  = quantile(esim,prob=.90)
  
  text_info = paste0(
    'Newton-Raphson Converged =  ',fit$converge,'\n',
    '# iterations             =  ',fit$niter,'\n',
    '(Unscaled) Log Lik       =  ',prettyNum(fit$Lik[1+fit$niter], digits=2),'\n',
    'Roughness Penalty        =  ',prettyNum(fit$Pen[1+fit$niter], digits=2),'\n\n',
    'Life Exp. 10%-90%ile CI  = [',prettyNum(e10,digits=3),'-',prettyNum(e90,digits=3),']'
  )
  
  
  
  mx_vals =  c(1,2,10,20,100,200,1000,2000,10000)
  
  this_plot =
    ggplot(data = df_single, aes(x=age,y=std)) +
    geom_line(aes(x=age,y=std), color='black', lwd=0.8) +
    geom_line(aes(x=age,y=logmx_fit), color=hue, lwd=1.5, alpha=.80) +
    geom_segment(data=df_grouped,aes(x=L,xend=U,
                                     y=logmx_obs,
                                     yend=logmx_obs),
                 color=hue,lwd=1, alpha=.90) +
    geom_ribbon(aes(x=age,ymin=Q10,ymax=Q90), fill=hue, alpha=.10) +
    labs(x='Age',y='Mortality Rate per 10,000 (Log Scale)',
         title=ti, subtitle = subti) +
    scale_x_continuous(breaks=c(0,1,seq(5,100,5)),minor_breaks = NULL) +
    scale_y_continuous(limits=range(c(-10,0,df_single$Q10,df_single$Q90)),
                       breaks=log(mx_vals /10000),
                       minor_breaks = NULL,
                       labels=paste(mx_vals)) +
    geom_text(x=55, y=log(3e-4), label=text_info, hjust=0, size=4,
              family='CO',face='bold') +
    theme_bw()
  
  if (!is.na(true_logm)) {
    this_plot = this_plot +
      geom_point(aes(x=0:99,y=true_logm), shape='+')
  } 
  
  print(this_plot)
} # show 

show(fit)